\documentclass[11pt]{report}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage[english]{babel}
\usepackage{chngcntr}
\addto\captionsenglish{%
  \renewcommand{\bibname}{References}
}

\begin{document}
\pagenumbering{roman}
\thispagestyle{empty}
\begin{center}
\vspace{4cm}
\doublespacing
\textbf{\Large A Deep Learning Approach to Forecast the Canadian Consumer Price Index (CPI) Using Encoder-Decoder Attention Mechanisms with Teacher Forcing Techniques }\\
\vspace{3cm}
\textbf{A Major Research Paper}\\
\textbf{presented to the Toronto Metropolitan University}\\
\vspace{1cm}
\textbf{in partial fulfillment of the requirements for the degree of}\\
\textbf{Master of Science in Data Science and Analytics}\\
\textbf{Department of Engineering}\\
\vspace{4cm}
\textbf{MRP Supervisor:}\\
Dr. Aliaa Alnaggar\\
\textbf{Second Reader:}\\
Dr. Ceni Babaoglu\\
\textbf{Candidate:}\\
Maame Cobbinah,\\
\vspace{0.1cm}
BSc Economics and Statistics, University of Victoria, 2017\\
\vspace{0.5cm}
{2022/2023}\\
\end{center}

\newpage
\pagenumbering{roman}
\begin{center}
\textbf{Author's Declaration for Electronic Submission of a Major Research Project (MRP)}
\end{center}
\doublespacing
I hereby declare that I am the sole author of this Major Research Paper. This is a true copy of the MRP, including any required final revisions.
I authorize Toronto Metropolitan University to lend this MRP to other institutions or individuals for the purpose of scholarly research.
I further authorize Toronto Metropolitan University to reproduce this MRP by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.
I understand that my MRP may be made electronically available to the public.\\
Maame Cobbinah
\newpage % Add a new page before the abstract
\begin{center}
\centering
\textbf{Abstract}\
\end{center}
\doublespacing
The Canadian Consumer Price Index (CPI) is an important macro-economic indicator that measures the average price change of goods and services purchased by households over time. It is one of the indicators commonly used by the Canadian government and the Bank of Canada for fiscal and monetary policies in areas such as adjusting interests rates, modifying consumer income payments for pensions and old age security, as well as providing cost-of-living wage adjustments for millions of Canadian workers. While traditional statistical methods have been useful in CPI forecasting, advancements in deep learning techniques and the availability of big data allow for more sophisticated and accurate forecasting methods. This MRP proposes a novel deep learning approach that utilizes Recurrent Neural Network (RNN) encoder-decoder attention mechanisms in forecasting the Canadian CPI. This method has shown promising results in various  forecasting applications and has the potential to enhance the accuracy and efficiency of CPI forecasting.

\vspace{1.5cm}
\textbf{Keywords:} \textit{CPI, Goods \& Services, Prices , Indicators, Forecasting, Statistical methods, Deep learning, Attention, RNN, Encoder, Decoder}
\newpage
\pagenumbering{roman}
\begin{center}
\textbf{ACKNOWLEDGEMENTS}
\end{center}
\doublespacing
I am deeply grateful to Dr. Aliaa Alnaggar, my supervisor, for her invaluable guidance and support during my Master's research project. Her expertise and insightful suggestions have been instrumental in shaping my work.
I am thankful for her continuous encouragement and feedback, which greatly contributed to the successful completion of my project. I am sincerely thankful for her unwavering support and mentorship throughout this journey.\\
Thank you,\\
Professor Aliaa Alnaggar
\newpage
\renewcommand{\contentsname}{TABLE OF CONTENTS} % Change the table of contents title
\tableofcontents
\newpage
\begin{center}
\pagenumbering{arabic}
\counterwithout{section}{chapter}
\setcounter{section}{0}
\section{Introduction}
\end{center}
\doublespacing
The development of CPI (Consumer Price Index) forecasting in macroeconomics has become crucial due to its significance in informing monetary policy decisions and guiding economic planning (Nyoni, 2019)~\cite{nyoni2019arima}. Accurate CPI forecasting helps policymakers and economists anticipate inflationary pressures, understand the dynamics of price changes, and assess the overall health of an economy. By providing insights into inflation trends, CPI forecasting enables policymakers to take proactive measures, such as adjusting interest rates and implementing appropriate fiscal policies, to maintain price stability and support sustainable economic growth. Concurrently, the growth in deep learning techniques has improved ability to forecast into the future. The integration of encoder-decoder attention mechanisms in Recurrent Neural Networks (RNNs) methods have shown promise in improving the accuracy and effectiveness of CPI forecasting models. The aim of this research paper is to explore the efficacy of this deep learning approach in predicting the Canadian Consumer Price Index, contributing to the existing literature on forecasting methods and providing valuable insights for policymakers and economists.
\vspace{0.1cm}\\
\subsection{Background}
The CPI is an economic indicator that measures the average cost variation in a diverse range of goods and services purchased by households, businesses, and other entities (Statistics Canada, 2023)~\cite{cpi2023}. It focuses specifically on items considered as consumer goods or services and is typically associated with retail prices. 
The CPI excludes expenditures, such as income taxes, charitable donations, pension contributions, and consumer savings and investments (Statistics Canada, 2023)~\cite{cpi2023}. 
\vspace{0.2cm}\\
On a monthly basis Statistics Canada gathers prices across a spectrum of good and services. The CPI is then determined by assigning weights to each item comparing the total expenditure on a specific item to the overall spending in Canada (Statistics Canada, 2023)~\cite{cpi2023}. These weights play a crucial role in assessing the impact of price changes on the overall consumer budget. 
\vspace{0.2cm}\\
The CPI is anchored to a time base, or base period, which represents the period when the index is assigned a value of 100. Currently, the CPI time base in Canada is set at 2002. When referencing specific index levels, it is customary to include the base period to provide context. For example, the CPI All-items for Canada in January 2020 was reported as 136.8 (2002=100), indicating that consumer prices were  \SI{36.8}{\percent} higher in January 2020 compared to the base period of 2002. To ensure an accurate representation of short-term inflation trends, the CPI incorporates adjustments for anticipated seasonal price fluctuations when comparing month-to-month changes in the index.
\vspace{0.2cm}\\
Recently, the Bank of Canada has relied on the CPI as a primary tool for quantitative tightening. This has resulted in several interest rate hikes, affecting the fees associated with mortgages, loans, and other forms of borrowing for Canadians. The bank's goal is to ultimately restore price stability and achieve balance in its balance sheet.
\vspace{0.2cm}\\
The CPI is a good inflation indicator as it calculated based on the representative basket of goods.  This basket is designed to reflect the consumption patterns of the average consumer, making it a representative sample of consumer spending. Secondly, the frequency of data collection and measurement for CPI helps to provide timely updates on inflation trends. This frequency allows policymakers and economists to monitor changes in consumer prices and make informed decisions accordingly.
\vspace{0.3cm}\\
Despite its usefulness, the CPI has some challenges in terms of measurement. For example, quality adjustments is very hard to measure. Although, the CPI aims to measure price changes while holding the quality of goods and services constant, the improvements in the quality of products over time, such as advancements in technology or changes in product features, are not always adequately accounted for in the index. This can result in an underestimation of the true cost of living. Secondly, it is hard to take geographical variation into account when looking at the aggregated index value. The CPI represents an average measure of inflation for the entire country. It may not capture regional variations in prices, where different areas experience different rates of inflation. For example, the cost of transporting food to Northern Canada turns to be higher and hence food prices are relatively higher in these areas compared to the rest of Canada. The aggregated gross value may hide these important details .
\vspace{1cm}\\
\subsection{Research Question}
The main objective of this research is to assess whether advanced deep learning techniques, specifically encoder-decoder attention mechanisms can outperform traditional time series models and other supervised machine learning algorithms  such as VAR,  ARIMA, Lasso Regressors, Support Vector Regressors, Random Forest Regressors and other baseline models when applied to macro-economic data for forecasting purposes.

\vspace{1.5cm}
\subsection{Independent/Dependent Variables}
The list of all the available fields in the dataset is given in Table 1.
\newpage 
\begin{center}
\section{Literature Review}
\end{center}
\setlength{\parindent}{0em} % Set paragraph indentation
The following section examines previous works in time series forecasting  in particular CPI and other economic/financial forecasting tasks and compares the performance of different models and techniques in these tasks.\\
\subsection{Traditional Forecasting Tasks}
Literature on CPI forecasting has mainly focused on traditional statistical techniques and basic machine learning methods. Atkeson \& Ohanian (2001)~\cite{atkeson2001phillips}, for example, examined such traditional techniques evaluating the effectiveness of Phillips curve-based models, specifically the NAIRU models, in forecasting CPI inflation. Through simulated exercises, it concluded that these models, including NAIRU Phillips curves, are not reliable tools for accurately predicting inflation, and alternative approaches should be explored to improve forecasting.\\
Similarly, Dondong (2010)~\cite{dongdong2010consumer} presented a simple Autoregressive Integrated Moving Average model. Dongdong's findings suggest that the ARIMA (12,1,12) model is effective in predicting the monthly CPI of China. However, this model suffers from data stationarity issues. Some CPI data often exhibit non-stationary behavior, such as trends or seasonal patterns. Although ARIMA performs better than Phillips curves in many cases it does not account for additional data transformations or differencing and hence can generate inaccurate output.\\
\subsection{Supervised Learning Forecasting Tasks}
As time progressed, supervised learning techniques emerged as better alternatives for forecasting CPI and other macroeconomic indicators. Medeiros et al. (2021)~\cite{medeiros2021forecasting} introduced the random forest and lasso regressor model as an alternative to forecast CPI. The results showed that the random forest regressor outperformed traditional techniques such as VAR and ARIMA across all evaluation metrics.
Wang et. al (2012)~\cite{wang2012new} proposed the Support Vector regressor as a better alternative for forecasting the Consumer Price Index (CPI). Their study concluded that using support vector regression (SVR) for CPI prediction is effective, as the nonlinear mapping capability of SVR allows for a better representation of the complex relationship between financial indicators and future CPI. However, the model did not incorporate time-varying weights, highlighting the need to explore alternative optimization methods for such cases.\\
\subsection{Hybrid Models}
Hybrid models have also become increasingly popular. Liu et al. (2023)~\cite{liu2023multi} proposed a hybrid model for CPI forecasting that combines Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN),  Empirical Mode Decomposition (EMD), Hierarchical Agglomerative (HAC), and the Independent Component Analysis (ICA). The model decomposes CPI data into adaptive modes using CEEMDAN and clusters them with HAC to reduce dimensions. ICA is then employed to extract hidden information, and NAR is used for forecasting each independent component. The hybrid model outperforms benchmark models such as the ARIMA and NAR models, showing lower MAE and RMSE values and higher R\textsuperscript{2} scores.\\
\subsection{Deep Learning Techniques}
Recently, deep learning techniques have emerged as better alternatives for forecasting. This due to their ability to handle complexity in time series data, perform transfer learning, and scalability. Bhardwaj (2022)~\cite{bhardwaj2022forecasting} proposed an Artificial Neural Network (ANN) method for forecasting GDP per capita, achieving an impressive R\textsuperscript{2} score of 0.9980 and a mean absolute error of 1.18\%. Nakamura (2005)~\cite{nakamura2005} compared a Neural Network (NN) model to autoregression (AR) models for CPI inflation forecasting, with the NN model outperforming in short-term forecasts. However, for longer forecast horizons, the NN model's performance becomes similar to the AR model. Barkan et al. (2022)~\cite{barkan2022forecasting} utilized a hierarchical recurrent neural network (HRNN) model to improve the prediction of disaggregated CPI inflation components, outperforming baseline models with an RMSE of 0.78 and correlation coefficients of 0.24 and 0.29. Theoharidi et al. (2023)~\cite{theoharidis2023deep} proposed a hybrid deep learning model, VAE-ConvLSTM, for inflation forecasting. It combined Variational Autoencoders (VAEs) and Convolutional LSTM Networks (ConvLSTM) to capture nonlinear patterns in inflation time series data. The model outperformed benchmark models, demonstrating superior consistency. These studies highlight the superior performance of deep learning models in economic forecasting.
\subsection{Transformer and Attention Models}
This MRP however focuses on attention models. The attention methodology was introduced by Vaswani et al (2017)~\cite{vaswani2017attention} in their paper "Attention is All You Need" . The model showcases the effectiveness of an attention Transformer model in language translation tasks. It achieved state-of-the-art results with improved efficiency, modeling long-range dependencies without recurrent or convolutional operations. The authors propose its applicability to forecasting tasks.\\
Abbasimehr \& Paki (2022)~\cite{abbasimehr2022improving} builds on this concept by applying attention to  several time series datasets. Their study applied a hybrid LSTM and multi-head attention model to 16 public datasets. The model utilized  a single-step ahead forecasting strategy with a selected lag size and optimized hyperparameters.The hybrid LSTM and multi-head attention model outperformed benchmarks, ranking first among the methods used.\\

Similarly, Chen (2022)~\cite{chen2022} incorporated attention mechanisms based on time and sequence characteristics to capture important influencing factors for financial markets. Their proposed Deep Neural Network model (MALSTM) outperformed classical time series models (ARIMA) and LSTM models, improving MAE from 8.96\% to 4.26\%. Integration of attention mechanisms and utilization of multiple time series inputs enhanced model effectiveness and robustness.
Li et al. (2018)~\cite{li2018stock} also introduced an attention-based multi-input LSTM model for stock price prediction. Their study focused on China's CSI-300 index and incorporated a dual-stage attention mechanism using a mainstream factor and three auxiliary factors. The MI-LSTM model achieved a significant 9.96\% improvement in mean square error compared to the LSTM model, demonstrating its effectiveness in stock price prediction.
Finally, Du et al. (2020)~\cite{du2020multivariate} proposed an effective approach for forecasting multi-step time series values using an attention-based encoder-decoder framework. The framework incorporates a bidirectional Long Short-Term Memory (Bi-LSTM) as the encoder, an LSTM as the decoder, and a temporal attention context layer. By leveraging the Bi-LSTM, the model captures deep temporal dependency features in multivariate time series. The authors evaluate their model on five public datasets and demonstrate its superiority over benchmark models, achieving better results in terms of RMSE and MAE.\\

The model used in this MRP shares a similar framework with the one proposed by Wang et al. (2022)~\cite{wang2022attention}. Their model employs an attention-based RNN encoder-decoder model for multi-step forecasting of sensory time series. The model captures interrelationships among heterogeneous parameters, employing spatial and parameter-wise attention layers in the encoders and temporal attention in the decoders for multi-step forecasting. However, there are notable differences in the approach adopted in this MRP versus Wang et al. (2022)~\cite{wang2022attention} . First, the model in this paper focuses on economic data (CPI) instead of sensory data (temperature). Additionally, we incorporate teacher forcing technique to facilitate information flow and mitigate the vanishing gradient problem they encountered in their framework. Furthermore, we introduce a normalization layer to enhance model stability and training convergence.


\newpage 
\begin{center}
\section{Exploratory Data Analysis}
\end{center}

The dataset used for this research provides monthly Consumer Price Index (CPI) values for different categories of goods and services. The dataset contains not seasonally adjusted CPI values for all-items, as well as categories such as food, shelter, clothing, transportation, and healthcare. The CPI indices are not seasonally adjusted as exploratory data analysis revealed that there was not much seasonality with the data .\\

\subsection{Data Acquisition}

The data for this project was acquired from Statistics Canada (\url{www150.statcan.gc.ca}). The datasets are open-sourced and compliant with the MRP requirements.

\subsection{Data Source \& Data Files}

The dataset was downloaded as csv files from the Statistics Canada website. The CPI values are reported as indices with the base year set to 2002=100. A subset of the data spanning from January 2003 to the present was extracted for the forecasting process. To facilitate the forecasting process, the data will be divided into training and test sets. The data files are posted on GitHub, and the link to the GitHub repository and a sample file are included in Appendix B.

\subsection{Data Structure Analysis}

The dataset includes fields such as the reference date period (grouped by month), CPI values for different categories of goods and services, and CPI values attributed to energy, goods, and services. Table 1 represents the data fields and their descriptions.
% ...


\begin{table}
  \centering
  \onehalfspacing
  \caption{Data Fields and Descriptions}
  \label{table:data-fields}
  \begin{tabular}{|l|p{0.6\linewidth}|}
    \hline
    \textbf{Field} & \textbf{Description} \\
    \hline
    Date & The reference date period for which the data is reported. The data is typically grouped by month. \\
    \hline
    All-Items & The values attributed to the basket of goods used to calculate the overall index. It includes all items, such as food, shelter, clothing, transportation, health care, recreation, and more. \\
    \hline
    Food & The values attributed to the basket of goods that encompass food items. This category includes food consumed at home and food purchased away from home. \\
    \hline
    Shelter & The values associated with housing and accommodation costs, such as rent, mortgage payments, and maintenance expenses. \\
    \hline
    Household operations, furnishings, \\ and equipment & The values attributed to household spending on daily operations, equipment, and furnishings. This category includes expenses like cleaning supplies, appliances, furniture, and other household items. \\
    \hline
    Clothing and footwear & The values attributed to the cost of clothing and footwear. This category includes clothing items, shoes, and related accessories. \\
    \hline
    Transportation & The values attributed to the cost of transportation, including vehicle purchases, fuel, vehicle maintenance, public transportation fares, and other transportation-related expenses. \\
    \hline
    Gasoline & The values associated with the price of gasoline. It reflects the changes in the cost of fuel used for transportation. \\
    \hline
    Health and personal care & The values attributed to health care and personal care expenses. This category includes medical services, prescription drugs, personal care products, and other related costs. \\
    \hline
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \onehalfspacing
  \label{table:data-fields}
  \begin{tabular}{|l|p{0.6\linewidth}|}
    \hline
    \textbf{Field} & \textbf{Description} \\
    \hline
    Recreation, education, and reading & The values attributed to education, recreation, and reading expenses. This category includes costs related to recreational activities, tuition fees, school supplies, books, and other educational materials. \\
    \hline
    Alcoholic beverages, tobacco products,\\ and cannabis & The values attributed to the cost of alcoholic beverages, tobacco products, and recreational cannabis. This category includes prices of alcoholic beverages, cigarettes, cigars, and other tobacco-related products. \\
    \hline
    All-Items excluding food and energy & The value attributed to all items except for the prices of food and energy. It provides an indication of overall trends excluding the impact of food and energy costs. \\
    \hline
    All-Items excluding energy & The value attributed to all items except for the cost of energy.It gives insight into the overall index while excluding the influence of energy prices.\\
    \hline
    Energy & The value attributed to the cost of energy, including electricity, natural gas, and other energy-related expenses. \\
    \hline
    Goods & The values attributed to the cost of goods, which includes both durable and non-durable goods. Durable goods are items with a longer lifespan, such as appliances and furniture, while non-durable goods are consumed quickly, such as food and clothing. \\
    \hline
    Services & The values attributed to the cost of services, including housing services, transportation services, medical services, and other services consumed by households. \\
    \hline
  \end{tabular}
\end{table}
% ...
\newpage % Add a new page before EDA
\subsection{Data Analysis and Statistics}
Our dataset consists of 244 rows and includes 1 dependent variable (All items category) and 15 independent variables. Summary statistics for each category are presented in Figure \ref{fig:summary-statistics}, providing insights into the distribution, central tendency, and variability of the data.
\vspace{0.5cm}
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{summary_statistics .png}
  \caption{Summary Statistics.}
  \label{fig:summary-statistics}
\end{figure}
\newpage 
In Figure 2, we use a time series plot to illustrate the trends of the various categories. Overall, CPI values have shown a consistent increase month over month, with the exception of clothing and footwear. Notably, categories such as gasoline, energy, and alcohol and cannabis exhibit high volatility, featuring significant peaks and exponential growth since 2003.
\vspace{0.5cm}
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{CPI line_plot.png}
  \caption{Canadian Consumer Price Index 2003 to 2023}
  \label{fig:CPI-lineplot}
\end{figure}
\newpage
Figure 3 provides an overview of the average Consumer Price Index (CPI) per product category. The data reveals that, on average, CPI values have increased by 23\% across all items. Notably, certain categories show even higher average increases. Gasoline, energy, alcohol, food, and shelter have experienced average increases of 60\%, 50\%, 44\%, 31\%, and 30\%, respectively.
These findings indicate that households, on average, allocate a significant portion of their spending—30\% or more—towards these specific products. Gasoline, energy, alcohol, food, and shelter play a substantial role in the average household budget, reflecting the impact of rising prices within these categories.
\vspace{0.5cm}
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{Category_breakdown.png}
  \caption{Average CPI by product category}
  \label{fig:category-breakdown barplot}
\end{figure}
\newpage
The distribution of the data is illustrated in Figure 4 through box plots. It is evident that most product categories have a lower quartile value greater than 100, except for clothing. This suggests that, compared to 2003, the Consumer Price Index (CPI) has increased for all categories except clothing.
The upper quartiles vary across different product categories. Notably, categories such as transportation, gasoline, and energy exhibit extreme outliers. These outliers represent data points that deviate significantly from the overall pattern or distribution of the dataset. They are observations that lie far away from the majority of the data points and can have a disproportionate impact on statistical measures and forecasting results.
During the modeling process, it may be necessary to remove these outliers from our training set to prevent them from causing forecasting errors. By eliminating these extreme values, we can ensure more accurate and reliable predictions.
\vspace{0.5cm}
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{box_plot.png}
  \caption{CPI Distribution per Product Category}
  \label{fig:distribution boxplot}
\end{figure}
\newpage
In Figure 5, we analyze the overall Consumer Price Index (All-Item Category) data using a heatmap. The heatmap reveals that there are no discernible patterns or variations in CPI values by month, indicating the absence of monthly seasonality. However, when observing the heatmap over a longer time frame, we observe a deepening of color, particularly in the last two years. This deepening color indicates a consistent growth in CPI compared to the base period of 2002.
These findings suggest that while there are no specific months with notably high CPI values indictive of seasonality, the overall trend of CPI has exhibited a steady increase since 2003, with a more pronounced growth in recent years.
\vspace{0.5cm}
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{heatmap.png}
  \caption{Heat Map of All Item Category}
  \label{fig:pattern-heatplot}
\end{figure}
\newpage
The ETS seasonal decomposition in Figure 6 further supports the observation that the data does not exhibit strong seasonality. The decomposition analysis reveals that the seasonal component is relatively weak or negligible.
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{ets decomposition.png}
  \caption{ETS Seasonal Decomposition }
  \label{fig:ets-decomposition}
\end{figure}
\newpage
Additionally, the Autocorrelation plot in Figure 7 demonstrates that the values in the "All-items" category are non-stationary. The plot illustrates a slow decay or gradual decline in autocorrelation values as the lag increases. This pattern is indicative of non-stationarity in the data
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\textwidth]{autocorrelation.png}
  \caption{Autocorrelation Overall CPI  }
  \label{fig:autocorrelation}
\end{figure}
These findings suggest that the dataset lacks strong seasonality and exhibits non-stationary behavior, as indicated by both the ETS seasonal  decomposition and the Autocorrelation plot.
\newpage
\begin{center}
\section{Problem Statement}
\end{center}
The problem of interest in this MRP is a multivariate time series problem. A multivariate time series problem involves analyzing a sequence of observations where each observation consists of multiple variables, aiming to capture dependencies and patterns to make predictions or forecasts (Bjerregaard et al., 2021)~\cite{bjerregaard2021}. Simply, the goal is to model future state based on existing historical data. In the case of the CPI data we anticipate the future value t+1 based on historical values of the endogenous and exogenous variables in the dataset. Mathematically we formulate our problem as :
\begin{align*}
\text{{CPI}}(t) = & \beta_0 + \beta_1 \cdot \text{{All-items}}(t-1) + \beta_2 \cdot \text{{Food}}(t-1) + \beta_3 \cdot \text{{Shelter}}(t-1) \\
& + \beta_4 \cdot \text{{Household operations}}(t-1) + \beta_5 \cdot \text{{Clothing and footwear}}(t-1) + \beta_6 \cdot \text{{Transportation}}(t-1) \\
& + \beta_7 \cdot \text{{Gasoline}}(t-1) + \beta_8 \cdot \text{{Health and personal care}}(t-1) \\
& + \beta_9 \cdot \text{{Recreation \& education}}(t-1) + \beta_{10} \cdot \text{{Alcohol, tobacco \& cannabis}}(t-1) \\
& + \beta_{11} \cdot \text{{All-items excluding food and energy}}(t-1) \\
& + \beta_{12} \cdot \text{{All-items excluding energy}}(t-1) + \beta_{13} \cdot \text{{Energy}}(t-1) \\
& + \beta_{14} \cdot \text{{Goods}}(t-1) + \beta_{15} \cdot \text{{Services}}(t-1) + \varepsilon(t)
\end{align*}
Where:
\begin{itemize}
  \item $CPI(t)$ represents the CPI value at time $t$ (the target variable).
  \item $All\text{-}items(t\text{-}1)$, $Food(t\text{-}1)$, $Shelter(t\text{-}1)$, and so on, represent the CPI values of the corresponding variables at time $t\text{-}1$ (the lagged values of the independent variables).
  \item $\beta_0, \beta_1, \beta_2, \ldots, \beta_{15}$ are the coefficients (parameters) of the model that need to be estimated.
  \item $\varepsilon(t)$ represents the error term at time $t$, which captures the unexplained variation in CPI that is not accounted for by the model.
\end{itemize}

\newpage 
\begin{center}
\section{Methodology}
\end{center}
In this paper, the proposed method for our forecasting problem is the Recurrent Neural Network (RNN) Encoder-Decoder model with self attention layers using teacher forcing technique. Before discussing the model, it is essential to provide a brief overview of the fundamental RNN architecture. RNNs are neural networks designed to model sequential data, where each value is influenced by preceding values (Mandic \& Chambers, 2001) ~\cite{Manic2001}. There are two types of RNNs commonly used: LSTM and GRU. 
Refer to Figure 8 for a basic depiction of RNN, LSTM, and GRU architectures.\\
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{lstm_rnn_gru.png}
  \caption{ Basic RNN LSTM GRU  (adapted from Danker, 2022 ~\cite{dancker2022})}
  \label{fig:lstm_rnn_gru}
\end{figure}\\
\subsection{RNN Attention Encoder Decoder Model}
The RNN Encoder-Decoder model is an advanced version of the recurrent neural network architecture first introduced by Cho et al.(2014) ~\cite{cho2014learning} .  The model's goal is to encode input sequences into a fixed-length vector representation, and then decode them to generate output sequences (Cho et al, 2014) ~\cite{cho2014learning}. The encoder component captures the input sequence information, while the decoder component generates the corresponding output sequence. The RNN Encoder-Decoder model has demonstrated significant success in various applications, such as machine translation, speech recognition, and text generation. Researchers have identified that the integration of attention layers can greatly improve this model. By incorporating attention layers, these models are liberated from the limitation of encoding input sequences into fixed-length vectors (Zhang et al.,2014)~\cite{zhang2018attention}. This advancement allows for more flexible and effective information processing, enhancing the model's ability to capture important dependencies across the input sequence. See Figure 9 for basic RNN Attention Encoder Decoder Architecture.\\
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{RNN_Encoder_Decoder.png}
  \caption{ RNN Attention Encoder Decoder Model (adapted from Brownlie, 2018 ~\cite{brownlee2018})}
  \label{fig:lstm_rnn_gru}
\end{figure}\\
Although the RNN Encoder-Decoder model has been applied to various forecasting tasks, such as earthquake size prediction by Li et al. (2022) ~\cite{lipan2022} and electricity load forecasting by Xiong et al. (2021) ~\cite{xiong2021}, it has not yet been applied to CPI data. Our proposed model stands out from existing literature not only due to its application to macroeconomic data but also because of the incorporation of attention layers and the introduction of the teacher forcing technique.
The teacher force technique involves using the true target sequence as input for the decoder, rather than using the predicted output from the previous time step (Deng \& Pan, 2021) ~\cite{deng2021}. Teacher forcing stabilizes the training process by providing accurate target values as inputs, thereby preventing error propagation and accelerating learning. It ensures more precise and informative inputs for the decoder, allowing the model to converge faster and improve gradient flow. See Figure 10 for basic architecture of RNN using Teacher Forcing techniques.
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{RNN_teacherforcing.png}
  \caption{ RNN with Teacher Forcing (adapted from Lungu, 2020 ~\cite{lungu2020})}
  \label{fig:lstm_rnn_gru}
\end{figure}\\

\newpage 
\begin{center}
\section{Experimentation}
\end{center}
\subsection{Aim of Study}
The aim of this study is to explore the potential of encoder decoder RNN attention architecture in improving the forecasting accuracy of Consumer price index. The model is compared with to baseline models such as Variational Autoregressive models, ARIMA models, Support Vector Regressors, Random Forest Regressors, Artificial Neural Network (ANN) architecture, and Convolutional LSTM architecture to assess its performance.
\subsection{Dependent and Independent Variables}
The independent variables in this experiment are: Food, Shelter, Household operations, Clothing and footwear category, Transportation, Gasoline, Health and personal care category, Recreation \& education, Alcohol, tobacco \& cannabis, All-items excluding food and energy, All-items excluding energy, Energy, Goods and Services. The dependent variable is the CPI index value associated with each category.
\subsection{Factors and Levels}
In this experiment, the factors are the different statistical models with different parameter settings that can validate the model algorithm and its evaluation metric performance.
\subsection{Experimental Design}
\subsubsection{Data Preprocessing}
This work utilizes monthly CPI data released by Statistics Canada. The dataset spans from January 2003 to April 2023 and was downloaded as a CSV file from the Stats Can website. The dataset had no missing values, but there were outliers which were removed before experimentation.
\subsubsection{Feature Selection and Experimental Design}
The data was split into training, validation, and test sets. For our proposed model, the training and validation set consist of the first 232 months, while the test set includes the last 12 months. Within the training and validation set, the first 80\% of the measurements are used for training, and the remaining 20\% are used for validation. \\
\subsubsection{Cross Validation}
To compare different forecasting models, a validation technique is implemented. Time series cross-validation is used in this project to measure accuracy using R2 score, SMAPE (Symmetric mean absolute percentage error), RMSE (Root Mean Square Error), and MAPE (Mean Absolute Percentage Error) between actual and predicted data
\subsection{Experiment Performance and Revisions}
A series of experiments were conducted to fine-tune the predicted values and improve the model's R2 values, root mean squared error, mean percentage error, and SMPAE. Details of the different experiments are provided below.
\vspace{1cm}
\subsubsection{Experiment 1: Proposed Model (Attention RNN Encoder Decoder Model).}
\textbf{Encoder:} \\
The proposed model begins with an encoder layer. The encoder part of the model takes the input sequence (in our case the CPI index values) as input. In our dataset this is a sequence of one-dimensional data. The input sequence is fed into an LSTM (Long Short-Term Memory) layer. The LSTM layer has 180 units and is set to return the sequences as well as the final hidden and cell states. The output of the encoder LSTM layer is stored in encoder\_outputs, and the final hidden and cell states are stored in state\_h and state\_c, respectively. These states capture the information from the input sequence and will be used as the initial states for the decoder.\\
\textbf{Decoder:} \\
The decoder part of the model takes the target sequence as input, which is also a sequence of one-dimensional data for our CPI data. During training, the decoder input sequence is the true target output sequence. The input sequence is passed through an LSTM layer with 180 units. The LSTM layer is set to return the sequences as well as the hidden and cell states. The output of the decoder LSTM layer is stored in decoder outputs, but the hidden and cell states are not used directly in this model. Instead, they are used to initialize the decoder states with the information captured by the encoder. This allows the decoder to generate an output sequence based on the input sequence. \\
\textbf {Attention Mechanism:}\\ 
The attention mechanism is a crucial component of this model. It helps the decoder focus on relevant parts of the input sequence during the generation of each output. In this implementation, the attention mechanism is implemented using the dot product attention. First, the dot product between the decoder outputs (decoder\_outputs) and the encoder outputs (encoder\_outputs) is computed using the Dot layer with axes=[2, 2]. This operation calculates the similarity between the decoder outputs and each encoder output. Next, an activation function (softmax) is applied to normalize the similarity scores across the encoder outputs. This step allows the model to assign higher weights to more relevant parts of the input sequence. The normalized attention weights are stored in the attention variable. The attention weights are then used to compute a weighted sum of the encoder outputs, giving more importance to the parts of the input sequence that the model pays more attention to. This is done using the Dot layer with axes=[2, 1] and stored in the context variable. Finally, the attention context vector and the decoder outputs are concatenated along the last axis using the Concatenate layer.\\
\textbf{Output and Training:}\\ 
The concatenated context vector and decoder output are passed through a TimeDistributed dense layer with linear activation, which produces the final output sequence. During training, instead of using the previous decoder output as input for the next step, the true target output sequence is fed into the decoder. This incorporates the teacher forcing technique. The model is compiled with the Adam optimizer and mean squared error (MSE) loss function. It is then trained using the training data and the corresponding target output for 500 epochs with a batch size of 4. After training, the model is evaluated on the test set. The test input sequences are fed into the model to generate predictions. The predictions are then compared with the target output sequence. 
See Figure 11 for visualization of model architecture.
\begin{figure}[htb]
  \centering
  \includegraphics[width=1.1\textwidth]{RNN_plot.png}
  \caption{RNN LSTM GRU }
  \label{fig:RNN Attention Encoder Decoder}
\end{figure}\\
\vspace{1cm}
\subsubsection{Experiment 2:  Baseline Models: Traditional Statistical Techniques}
In this section, we compare our proposed model to the  ARIMA and VAR models. Dondong (2010)~\cite{dongdong2010consumer} advocated for the use of ARIMA and VAR models as effective models for CPI forecasting.\\
\textbf{Variational Autoencoder Model:}\\
For the Variational Autoencoder baseline model, the dataset was divided into a training set (232 months) and a test set (12 months). The VAR model was trained on the training set using the VAR class, and the optimal lag order was determined using the  Akaike Information Criterion (AIC). Autocorrelation and partial autocorrelation plots were used to analyze residuals, and CPI was forecasted for the test set.\\
\textbf{ARIMA Model:}\\
For the ARIMA model, the dataset was also divided into a training set (232 months) and a test set (12 months). After conducting parameter tuning, the ARIMA model was specified with an order of (1, 1, 1). Stationarity was checked using the ADF test, and ARIMA models with the specified order were fitted for each component of the CPI data. Autocorrelation and partial autocorrelation plots were generated to analyze residuals, and forecasting was performed on the test set.
\vspace{1cm}
\subsubsection{Experiment 3:  Baseline Models: Supervised Learning Techniques}
In this section, we discuss the implementation of the Lasso regressor, Support Vector Regressor (SVR), and Random Forest Regressor models, which serve as benchmarks against our proposed architecture.\\
\textbf{Lasso Regressor:}\\
In the Lasso regressor experiment, the dataset was split into a training set (220 months) and a test set (24 months). Sequential values were used for training, and a Lasso regressor model was created with an alpha value of 0.1. Multiple Lasso regressors were combined using stacking, which integrated their predictions for the final forecast. The stacking regressor was trained on the scaled training data and used to predict values on the scaled testing data.\\
\textbf{Support Vector Regressor:}\\
For the Support Vector Regressor experiment, the dataset was divided into a training set (220 months) and a test set (24 months). The Support Vector Regressor (SVR) method was applied separately to each CPI component. The training data, consisting of sequential values, was reshaped into a 2D array to fit the SVR model. Subsequently, predictions were generated for the test data using the trained SVR models. The forecasts from each SVR model were combined using averaging to create an ensemble forecast. Finally, the ensemble forecast was used to predict values for the test set.\\
\textbf{Random Forest Regressor:}\\
In the Random Forest Regressor experiment, the dataset was split into a training set and a test set. Each CPI component was trained individually using the first 220 months for training and the last 24 months for testing. Sequential integers were used to represent the training data. A Random Forest Regressor model was created and used to predict values on the test set. The model's residuals, representing the difference between actual and predicted values, were analyzed through autocorrelation and partial autocorrelation plots. 
\vspace{1cm}
\subsubsection{Experiment 4:  Baseline Models: Deep Learning Techniques}
In this section, experiments are conducted using the Artificial Neural Network (ANN) and Convolutional LSTM models as baselines to compare to our proposed architecture.\\
\textbf{Artificial Neural Network (ANN) Model:}\\
In the implementation of the Artificial Neural Network (ANN) experiment, the dataset was divided into training and test sets. The training set contained 80\% of the CPI indices, while the remaining 20\% formed the test set. The ANN model was constructed using TensorFlow's Sequential API, consisting of three dense layers with 120 units each. All layers, including the output layer, utilized the ReLU activation function. The model was compiled with the Adam optimizer and the mean squared error (MSE) loss function. Training was performed for 500 epochs with a batch size of 8. Subsequently, the trained model was used to make predictions on the test set.\\
\textbf{Convolutional LSTM Model:}\\
For this implementation, the data was split into training and testing sets. The ConvLSTM model was constructed using TensorFlow's Sequential API. It consisted of a ConvLSTM2D layer with 120 units, followed by a Flatten layer, a fully connected Dense layer with 120 units, and a final Dense layer with a number of units equal to the target columns. The model was compiled with the Adam optimizer and mean squared error loss. Training was conducted using the fit function for 300 epochs with a batch size of 2. After training, the model was evaluated on the test data to compute the loss value, and predictions were made on the test data using the trained model.

\newpage 
\begin{center}
\section{Results}
\end{center}
\subsection{Model Evaluation}
The results of our proposed and baseline models were evaluated using the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), SMAPE (Symmetric Mean Absolute Percentage Error) and R-squared (R2).\\
\textbf{Root Mean Squared Error (RMSE):}\\
The RMSE mathematically formulated as 
\begin{align*}
\text{RMSE} &= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\hat{y_i} - y_i)^2}
\end{align*}
Is used provide a measure of the model's accuracy by quantifying the average squared differences between the predicted values and the actual values . The RMSE is preferred evaluation metric because it is sensitive to outliers  making it a useful metric for identifying and assessing the impact of extreme predictions.\\
\textbf{Mean Absolute Error (MAE):}\\
The MAE mathematically formulated as 
\begin{align*}
{\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |\hat{y_i} - y_i|}
\end{align*}
calculates the average absolute difference between the predicted values and the actual values. The MAE is less sensitive to outliers and hence  makes it a better choice when the data contains extreme outliers that could significantly impact the prediction results.\\
\textbf{Symmetric Mean Absolute Percentage Error (SMAPE):}\\
The SMAPE mathematically formulated as 
\begin{align*}
\text{SMAPE} = \frac{1}{n} \sum_{i=1}^{n} \frac{| \hat{y_i} - y_i |}{(|\hat{y_i}| + |y_i|)/2} \times 100
\end{align*}
is also used to assess the relative accuracy of a model's predictions. It calculates the percentage difference between the predicted values and the actual values in a symmetric manner. It is a preferred evaluation metric because both overestimation and underestimation errors are treated equally.\\
\textbf{R-squared (R2):}\\
Finally, the R2 mathematically formulated as 
\begin{align*}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{align*}

 measures the proportion of the variance in the dependent variable (in our case CPI indices) that is predictable from the independent variables (CPI predictions). It is a preferred metric due to its goodness of fit nature as well as interpretability. It provides a measure of how well the model fits the data. A higher R2 value indicates that a larger proportion of the variance in the data is accounted for by the model. Again, R2 has a straightforward interpretation. A value of 1 indicates that the model perfectly fits the data, while a value of 0 means that the model provides no  improvement \vspace{1cm} over a simple mean-based prediction.\\ 
\noindent
\subsection{Model Results}
In \textbf{Table 2} we see the overall results/performance on each  experimental model.
\begin{table}[htbp]
  \centering
  \renewcommand{\arraystretch}{1.5} % Set row spacing to 1.5
  % Use tabularx to adjust the table width
  \begin{tabularx}{0.9\textwidth}{l *{4}{S[table-format=3.2]}}
    \toprule
    Model & {RMSE} & {MAE} & {SMAPE} & {R2} \\
    \midrule
    \textbf{RNN Attention Encoder Decoder Model} & \textbf{0.15} & \textbf{0.09} & \textbf{0.07} & \textbf{0.99} \\
    VAR & 20.88 & 16.72 & 0.08 & 0.29 \\
    ARIMA & 9.13 & 5.56 & 3.12 & -4.19 \\
    Lasso Regressor (LR) & 18.82 & 11.37 & 0.81 & -2.03 \\
    Random Forest Regressor (RFR) & 15.37 & 14.49 & 23.85 & -8.16 \\
    Support Vector Regressor (SVR) & 19.18 & 12.08 & 7.21 & -2.71 \\
    ANN & 17.47 & 9.67 & 6.07 & 0.37 \\
    Conv-LSTM & 127.22 & 126.14 & 198.5 & -129.85 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance Comparison Between Proposed vs Baseline Methods}
  \label{tab:evaluation}
\end{table}
\newpage
\noindent
From \textbf{Table 2} we can see that RNN Attention Encoder Decoder Model significantly outperforms the other forecasting models for CPI (Consumer Price Index) predictions. It achieved the lowest RMSE, MAE, and SMAPE values, indicating smaller prediction errors and more balanced performance. Additionally, the R2 score of 0.99 demonstrates a high level of goodness of fit, indicating that around 99\% of the variance in the CPI data can be accurately explained by the model's predictions. In contrast, the other models showed higher errors and lower R2 scores, indicating weaker  predictive \vspace{1cm} capabilities. \\
\noindent
We also analyzed prediction performance of the proposed RNN Attention Encoder Decoder Model compared to the baseline model at a  6-time step interval using graphical display.
In Figure 8, the RNN Attention Encoder Decoder Model consistently predicts more accurately and closer to the actual values compared to other baseline models.
The deviation between the predicted value and the ground-truth value for the proposed model is not too large, while more obvious deviations are observed in the VAR, Lasso Regressor, RFR, SVR, and Convolutional LSTM Models. For instance, when examining the predictions for April 2023, the actual CPI value is 156. The proposed model predicts a CPI value of 155, whereas the baseline models predict 163 using VAR, 153 with ARIMA, 144 with Lasso Regressor, 153 with Random Forest Regressor, 138 with Support Vector Regressor, 154 with the ANN model, and 133 with the Convolutional LSTM model. This exemplifies that the proposed model predicts much closer to the actual values.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{RNNpredictions.png}
    \caption{RNN Attention Encoder Decoder Model}
    \label{fig:plot1}
  \end{subfigure}
  \hspace{0.5cm}
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{VARpredictions.png}
    \caption{VAR Model}
    \label{fig:plot2}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ARIMApredictions.png}
    \caption{ARIMA Model}
    \label{fig:plot3}
  \end{subfigure}
  \hspace{0.5cm}
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Lassopredictions.png}
    \caption{Lasso Regressor Model}
    \label{fig:plot4}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{RFpredictions.png}
    \caption{Random Forest Regressor Model}
    \label{fig:plot5}
  \end{subfigure}
  \hspace{0.5cm}
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SVRpredictions.png}
    \caption{Support Vector Regressor Model}
    \label{fig:plot6}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ANNpredictions.png}
    \caption{ANN Model}
    \label{fig:plot7}
  \end{subfigure}
  \hspace{0.5cm}
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ConvLSTMpredictions.png}
    \caption{Convolutional LSTM Model}
    \label{fig:plot8}
  \end{subfigure}

  \caption{Model Experiment with 6 time steps  for Actual vs Prediction}
  \label{fig:grid}
\end{figure}

\newpage
\noindent
In  \textbf{Table 3} we see performance for RNN Encoder Decoder Moder for each CPI Indicator 

\begin{table}[htbp]
  \centering
  \renewcommand{\arraystretch}{1.5} % Set row spacing to 1.5
  \begin{tabular}{l S[table-format=1.6] S[table-format=1.6] S[table-format=1.6] S[table-format=1.6]}
    \toprule
    CPI Indicator & {RMSE} & {MAE} & {SMAPE} & {R2 Score} \\
    \midrule
    All-items & 0.437936 & 0.197009 & 0.14285 & 0.999038 \\
    Food & 0.161524 & 0.138011 & 0.101987 & 0.999941 \\
    Shelter & 0.163271 & 0.140608 & 0.109276 & 0.999905 \\
    Household operations & 0.155238 & 0.137616 & 0.123686 & 0.999766 \\
    Clothing and footwear & 0.153369 & 0.103751 & 0.111492 & 0.99625 \\
    Transportation & 0.186686 & 0.143888 & 0.109012 & 0.999884 \\
    Gasoline & 1.076951 & 0.293529 & 0.135658 & 0.999174 \\
    Health and personal care & 0.149622 & 0.126032 & 0.107032 & 0.99982 \\
    Recreation \& education & 0.111623 & 0.10269 & 0.094262 & 0.999803 \\
    Alcohol, tobacco \& cannabis & 0.13105 & 0.111033 & 0.077413 & 0.999967 \\
    All-items excluding food and energy & 0.142433 & 0.130271 & 0.111262 & 0.999859 \\
    All-items excluding energy & 0.118856 & 0.106983 & 0.088302 & 0.999923 \\
    Energy & 0.18413 & 0.154158 & 0.098693 & 0.999954 \\
    Goods & 0.137075 & 0.126854 & 0.110661 & 0.999824 \\
    Services & 0.133564 & 0.112557 & 0.085544 & 0.999946 \\
    \bottomrule
  \end{tabular}
  \caption{RNN Attention Encoder Decoder Model Evaluation Results}
  \label{tab:cpi_results}
\end{table}
\\
\noindent
From the above, the model exhibits exceptional performance across various CPI indicators.  Food, Shelter, Household operations, and Transportation, predictions demonstrated remarkable accuracy, with R2 scores close to 1. The model's impressive performance extends to various other CPI components, including Health and personal care, Recreation \& education, and Alcohol, tobacco \& cannabis. In each case, the model achieved low prediction errors and high R2 scores, indicating its reliable forecasting capabilities across diverse sectors of the economy. The RNN Attention Encoder Decoder Model also proved valuable for forecasting overall inflation trends. When considering All-items, All-items excluding food and energy, and All-items excluding energy, the model produced precise predictions, as reflected by their R2 scores approaching 1. Although the Gasoline CPI category exhibited slightly higher errors in the RMSE, the model's R2 score of 0.999 still suggests that it captures a substantial proportion of the variance in this indicator. Overall, the predictions on the various indicators were good.

\subsection{Model Forecast}
In Table 4 we have our 6 month forecast from our proposed model for the  various CPI indicators. 
\begin{table}[htbp]
  \centering
  \renewcommand{\arraystretch}{1.5} % Set row spacing to 1.5
  \small % Set font size to small
  \begin{tabularx}{\textwidth}{X *{6}{c}}
    \toprule
    CPI Index & May/23 & Jun/23  & Jul/23 & Aug/23 & Sep/23 & Oct/23  \\
    \midrule
    All-items & 148.89 & 159.74 & 162.93 & 169.06 & 163.70 & 163.99 \\
    Food & 173.16 & 186.22 & 189.28 & 190.04 & 190.25 & 190.34 \\
    Shelter & 165.42 & 175.19 & 177.76 & 178.46 & 178.74 & 178.88 \\
    Household operations & 129.76 & 140.08 & 142.68 & 143.49 & 143.84 & 144.03 \\
    Clothing \& footwear & 95.60 & 96.80 & 97.50 & 97.90 & 96.20 & 98.20 \\
    Transportation & 161.49 & 170.91 & 174.30 & 175.82 & 176.71 & 177.28 \\
    Gasoline & 216.31 & 253.18 & 261.73 & 263.78 & 264.34 & 264.58 \\
    Health and personal care & 141.14 & 152.70 & 156.26 & 157.14 & 157.38 & 157.49 \\
    Recreation, education and reading & 123.70 & 136.76 & 140.07 & 140.99 & 141.31 & 141.49 \\
    Alcohol, tobacco \& cannabis & 184.59 & 199.54 & 204.80 & 206.08 & 206.45 & 206.60 \\
    All-items excluding food \& energy & 139.18 & 151.90 & 155.37 & 156.23 & 156.54 & 156.70 \\
    All-items excluding energy & 150.13 & 162.29 & 165.52 & 166.33 & 166.56 & 166.67 \\
    Energy & 198.23 & 235.26 & 245.23 & 247.66 & 248.18 & 248.35 \\
    Goods & 135.63 & 153.57 & 156.92 & 157.80 & 158.11 & 158.27 \\
    Services & 160.13 & 169.22 & 172.57 & 173.71 & 174.26 & 174.59 \\
    \bottomrule
  \end{tabularx}
  \caption{CPI Forecasted Values for May 2023, to October 2023}
  \label{tab:cpi_data}
\end{table}

Our predictions suggest that CPI values for all indices, except for Clothing and Footwear, are expected to increase over the next two quarters, indicating the likelihood of inflationary pressures. In response, monetary authorities, such as the Bank of Canada, may consider implementing a tighter monetary policy by raising interest rates. This measure can help curb inflation by reducing borrowing and spending, potentially stabilizing prices. Additionally, sectors like Gasoline and Energy are projected to experience significant price increases, warranting targeted policies to address sector-specific inflationary pressures, such as promoting energy efficiency or providing subsidies to mitigate consumer impact.\\
\noindent
The forecasted rise in CPI for Food and other essential items may lead to increased living costs for households. Fiscal authorities can consider implementing measures to support vulnerable populations, such as food assistance programs, or direct cash transfers.\\
\noindent
While the forecasted trends provide valuable directional cues for policy decisions, it's important to acknowledge that actual outcomes may be subject to uncertainties. Policymakers can use these projections as a basis for formulating flexible and adaptive policies that respond to evolving economic conditions.


\newpage 
\begin{center}
\section{Conclusion}
\end{center}
In this paper, we proposed an advanced model aimed at providing a more accurate forecast for inflation indicators, such as the CPI. 
The model leverages an Attention-based RNN Encoder-Decoder architecture with the Teacher Forcing technique.
The model was designed to bridge the gap between capturing long-term dependencies in CPI data and effectively handle the non-linearity commonly present in such datasets. 
Additionally, the application of the Teacher Forcing technique served to stabilize and accelerate the model training process while mitigating issues related to error propagation.\\
\indent The results of our study demonstrated the outstanding performance of the RNN Attention Encoder-Decoder Model, surpassing both traditional approaches and various supervised and deep learning techniques. 
The model achieved an impressively low RMSE of 0.15, indicating its ability to generate predictions close to the actual CPI values. 
Furthermore, the high R2 value of 0.99 illustrated the model's superior predictive power, explaining almost all of the variance in the CPI data.\\
While this research has achieved promising results, it is essential to acknowledge its limitations. For example interpertability with attention based decoder encoder mechanisms can be challenging that is understanding exactly how the model makes its decisions and which input elements are most influential in the predictions might not be straightforward. Hence it is very difficult to do any kind of backward engineering.
Furthermore, while the model performs well with local or regional data, it may face challenges in capturing context or dependencies between distant elements in a global dataset.\\
\indent Regardless of its limitations the findings of this research make a significant contribution to the field of economic forecasting and underscore the potential of employing advanced deep learning techniques for CPI prediction.
Moreover, this study opens avenues for future research on economic forecasting, including exploring other transformer methods, attention mechanisms, and hyperparameter tuning techniques such as Scheduled Sampling, which shares similarities with Teacher Forcing.
As the world continues to address the current inflation crises, the knowledge gained from this study can contribute to making informed decisions and developing effective policies to ensure economic stability and growth.


\begin{thebibliography}{9}
\bibitem{nyoni2019arima}
\href{https://mpra.ub.uni-muenchen.de/92442/}Nyoni, T. (2019). ARIMA modeling and forecasting of Consumer Price Index (CPI) in Germany. Federal Reserve Bank of St Louis.
\bibitem{cpi2023}
Statistics Canada. (2023). Consumer Price Index Portal. Retrieved from \url{https://www.statcan.gc.ca/en/subjects-start/prices_and_price_indexes/consumer_price_indexes}
\bibitem{atkeson2001phillips}
\href{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=63940a2734d944b28c6ae8ad3ae69ea9ab3b317f}Atkeson, A., \& Ohanian, L. E. (2001). Are Phillips Curves Useful for Forecasting Inflation? Federal Reserve Bank of Minneapolis.
\bibitem{dongdong2010consumer}
\href{https://ieeexplore.ieee.org/abstract/document/5571115}Dongdong, W. (2010). The Consumer Price Index Forecast Based on ARIMA Model. 1, 307–310.
\bibitem{medeiros2021forecasting}
\href{https://www.tandfonline.com/doi/abs/10.1080/07350015.2019.1637745}Medeiros, M. C., Vasconcelos, G. F. R., Veiga, Á., \& Zilberman, E. (2021). Forecasting Inflation in a Data-Rich Environment: The Benefits of Machine Learning Methods. Journal of Business \& Economic Statistics, 39(1), 98–119.
\bibitem{wang2012new}
\href{https://www.sciencedirect.com/science/article/pii/S1877050912002591}Wang, Y., Wang, B., \& Zhang, X. (2012). A New Application of the Support Vector Regression on the Construction of Financial Conditions Index to CPI Prediction. Procedia Computer Science, 9, 1263–1272.
\bibitem{liu2023multi}
\href{https://www.sciencedirect.com/science/article/abs/pii/S037843712200927X}Liu, J., Ye, J., \& E, J. (2023). A multi-scale forecasting model for CPI based on independent component analysis and non-linear autoregressive neural network. Physica A, 609, 128369.
\bibitem{bhardwaj2022forecasting}
\href{https://ieeexplore.ieee.org/abstract/document/9791714} Bhardwaj, V., Bhavsar, P., \& Patnaik, D. (2022). Forecasting GDP per capita of OECD countries using machine learning and deep learning models. 1–6.
\bibitem{nakamura2005}
\href{https://www.sciencedirect.com/science/article/abs/pii/S0165176504003088}Nakamura, E. (2005). Inflation forecasting using a neural network. Economics Letters, 86(3), 373–378.
\bibitem{barkan2022forecasting}
\href{https://www.sciencedirect.com/science/article/pii/S0169207022000607}Barkan, O., Benchimol, J., Caspi, I., Cohen, E., Hammer, A., \& Koenigstein, N. (2022). Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks. International Journal of Forecasting.
\bibitem{theoharidis2023deep}
\href{https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2757}Theoharidis, A. F., Guillén, D. A., \& Lopes, H. (2023). Deep learning models for inflation forecasting. Applied Stochastic Models in Business and Industry.
\bibitem{vaswani2017attention}
\href{https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \& Polosukhin, I. (2017). Attention Is All You Need.
\bibitem{abbasimehr2022improving}
\href{https://link.springer.com/article/10.1007/s12652-020-02761-x} Abbasimehr, H., \& Paki, R. (2022). Improving time series forecasting using LSTM and attention models. Journal of Ambient Intelligence and Humanized Computing, 13(1), 673–691.
\bibitem{chen2022}
\href{https://www.proquest.com/docview/2646636425?parentSessionId=wWqhlRqQUvmO07Pob1QDmfWhjQxzr0SIB6JWzeupwxo%3D&pq-origsite=summon&accountid=13631} Chen, F. (2022). Deep Neural Network Model Forecasting for Financial and Economic Market. \textit{Journal of Mathematics (Hidawi)}, 2022, 1--10.
\bibitem{li2018stock}
\href{http://proceedings.mlr.press/v95/li18c.html}Li, H., Shen, Y., \& Zhu, Y. (2018). Stock Price Prediction Using Attention-based Multi-Input LSTM. Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China
\bibitem{du2020multivariate}
\href{https://www.sciencedirect.com/science/article/abs/pii/S0925231220300606}Du, S., Li, T., Yang, Y., \& Horng, S.-J. (2020). Multivariate time series forecasting via attention-based encoder–decoder framework. Neurocomputing (Amsterdam), 388, 269–279.
\bibitem{wang2022attention}
\href{https://www.sciencedirect.com/science/article/pii/S1877050922015538}Wang, Y., Li, T., Lu, W., \& Cao, Q. (2022). Attention-inspired RNN Encoder-Decoder for Sensory Time Series Forecasting. Procedia Computer Science, 209, 103–111.
\bibitem{Manic2001}
\href{https://orca.cardiff.ac.uk/id/eprint/31639}
Mandic, D., \& Chambers, J. (2001). Recurrent neural networks for prediction: learning algorithms, architectures and stability. Wiley.
\bibitem{lipan2022}
\href{https://ieeexplore.ieee.org/abstract/document/9328806}
Li, T., Pan, Y., Tong, K., Ventura, C. E., \& de Silva, C. W. (2022). Attention-Based Sequence-to-Sequence Learning for Online Structural Response Forecasting Under Seismic Excitation. IEEE Transactions on Systems, Man, and Cybernetics. Systems, 52(4), 2184–2200.
\bibitem{xiong2021}
\href{https://ieeexplore.ieee.org/abstract/document/9637992}
Xiong, J., Zhou, P., Chen, A., \& Zhang, Y. (2021). Attention-based Neural Load Forecasting: A Dynamic Feature Selection Approach. 01–05.
\bibitem{bjerregaard2021}
\href{https://www.sciencedirect.com/science/article/pii/S2666546821000124}
Bjerregård, M. B., Møller, J. K., \& Madsen, H. (2021). An introduction to multivariate probabilistic forecast evaluation. Energy and AI, 4, 100058.
\bibitem{cho2014learning}
\href{https://arxiv.org/abs/1406.1078}
Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., \& Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078ces
\bibitem{zhang2018attention}
\href{https://link.springer.com/chapter/10.1007/978-3-030-01424-7_18}
Zhang, D., Fang, Z., Cao, Y., Liu, Y., Chen, X., \& Tan, J. (2018). Attention-based RNN model for joint extraction of intent and word slot based on a tagging strategy. In Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27 (pp. 178-188). Springer International Publishing.
\bibitem{dancker2022}
\href{https://towardsdatascience.com/a-brief-introduction-to-recurrent-neural-networks-638f64a61ff4}
Dancker, J. (2022). A Brief Introduction to Recurrent Neural Networks: An introduction to RNN, LSTM, and GRU and their implementation. Retrieved from https://towardsdatascience.com/a-brief-introduction-to-recurrent-neural-networks-638f64a61ff4
\bibitem{brownlee2018}
\href{https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/}
Brownlee, J. (2018). How to Configure an Encoder-Decoder Model for Neural Machine Translation. Retrieved from https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/
\bibitem{deng2021}
\href{https://www.sciencedirect.com/science/article/pii/S0920410520310366}
Deng, L., \& Pan, Y. (2021). Data-driven proxy model for waterflood performance prediction and optimization using Echo State Network with Teacher Forcing in mature fields. Journal of Petroleum Science \& Engineering, 197, 107981.
\bibitem{lungu2020}
\href{http://www.clungu.com/Teacher-Forcing/}
Lungu, C. (2020). Machine Learning Blog: Training an RNN with teacher forcing. Retrieved from \url{http://www.clungu.com/Teacher-Forcing/}
\end{thebibliography}
\end{document}
